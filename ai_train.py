# ai_train.py
import sqlite3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
import joblib
import os
from datetime import datetime

DB_FILE = 'bot3_stats.db'
MODEL_FILE = 'ai_model.pkl'
ENCODER_FILE = 'suit_encoder.pkl'

def prepare_features(conn):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–≥—Ä—ã
    games_df = pd.read_sql_query('''
        SELECT game_num, left_suits, right_suits, has_r, has_x, is_tie 
        FROM games 
        ORDER BY game_num
    ''', conn)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    preds_df = pd.read_sql_query('''
        SELECT pred_id, source_game, target_game, suit, result, attempt 
        FROM predictions 
        WHERE result IS NOT NULL
    ''', conn)
    
    if len(preds_df) < 50:
        print(f"‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(preds_df)} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤. –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 50.")
        return None, None
    
    # –°–æ–∑–¥–∞—ë–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
    X = []
    y = []
    feature_names = []
    
    # –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –º–∞—Å—Ç–µ–π
    le = LabelEncoder()
    all_suits = ['‚ô•Ô∏è', '‚ô†Ô∏è', '‚ô£Ô∏è', '‚ô¶Ô∏è']
    le.fit(all_suits)
    joblib.dump(le, ENCODER_FILE)
    
    for _, pred in preds_df.iterrows():
        features = []
        
        # –ù–∞—Ö–æ–¥–∏–º –∏—Å—Ö–æ–¥–Ω—É—é –∏–≥—Ä—É
        source_game = games_df[games_df['game_num'] == pred['source_game']]
        if len(source_game) == 0:
            continue
            
        # –ü—Ä–∏–∑–Ω–∞–∫ 1: –º–∞—Å—Ç—å –≤ –∏—Å—Ö–æ–¥–Ω–æ–π –∏–≥—Ä–µ
        if source_game.iloc[0]['right_suits']:
            right_suits = source_game.iloc[0]['right_suits'].split(',')
            if len(right_suits) > 0:
                features.append(le.transform([right_suits[0]])[0])
            else:
                features.append(-1)
        else:
            features.append(-1)
        
        # –ü—Ä–∏–∑–Ω–∞–∫ 2: –±—ã–ª –ª–∏ #R –≤ –∏—Å—Ö–æ–¥–Ω–æ–π –∏–≥—Ä–µ
        features.append(source_game.iloc[0]['has_r'])
        
        # –ü—Ä–∏–∑–Ω–∞–∫ 3: –±—ã–ª –ª–∏ #X
        features.append(source_game.iloc[0]['has_x'])
        
        # –ü—Ä–∏–∑–Ω–∞–∫ 4: –Ω–∏—á—å—è?
        features.append(source_game.iloc[0]['is_tie'])
        
        # –ü—Ä–∏–∑–Ω–∞–∫ 5: –Ω–æ–º–µ—Ä –ø–æ–ø—ã—Ç–∫–∏
        features.append(pred['attempt'])
        
        # –ü—Ä–∏–∑–Ω–∞–∫ 6: —Ü–µ–ª–µ–≤–∞—è –º–∞—Å—Ç—å (–∫–æ—Ç–æ—Ä—É—é –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º)
        target_suit = pred['suit']
        features.append(le.transform([target_suit])[0])
        
        X.append(features)
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: –∑–∞—à—ë–ª (1) –∏–ª–∏ –Ω–µ—Ç (0)
        y.append(1 if pred['result'] == 'win' else 0)
    
    feature_names = [
        'source_suit', 'has_r_source', 'has_x_source', 
        'is_tie_source', 'attempt_num', 'target_suit'
    ]
    
    return np.array(X), np.array(y), feature_names, le

def train_model():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    print(f"\n{'='*50}")
    print("ü§ñ AI –û–ë–£–ß–ï–ù–ò–ï –ë–û–¢–ê 3")
    print(f"{'='*50}")
    
    if not os.path.exists(DB_FILE):
        print("‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    conn = sqlite3.connect(DB_FILE)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X, y, feature_names, le = prepare_features(conn)
    conn.close()
    
    if X is None:
        return
    
    print(f"\nüìä –î–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(X)} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")
    print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_names)}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ XGBoost
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss'
    )
    
    model.fit(X_train, y_train)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    
    print(f"\nüìà –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏:")
    print(f"   Train accuracy: {train_score:.3f}")
    print(f"   Test accuracy:  {test_score:.3f}")
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    importance = model.feature_importances_
    print(f"\nüîç –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for name, imp in sorted(zip(feature_names, importance), key=lambda x: -x[1]):
        print(f"   {name}: {imp:.3f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    joblib.dump(model, MODEL_FILE)
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_FILE}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
        'train_date': datetime.now().isoformat(),
        'train_samples': len(X),
        'train_accuracy': train_score,
        'test_accuracy': test_score,
        'feature_names': feature_names
    }
    joblib.dump(metadata, 'ai_metadata.pkl')
    print(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

if __name__ == "__main__":
    train_model()